{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with 3 datasets, the image and archive datasets were pretty neat and clean ready to download, no problem in the process. To aquire the tweeter api data I created a developer account and with the keys provided I had all the tools necessary to download the data. In the process I set the wait on rate limit to True to finish the download non stop and chose the 'extended' option of tweets data. At first I didn't use 'try' function and got multiple errors due to deleted tweets but with by adding it to my code I finished the download with no problem. Then I extracted the json values and put them in a list and saved that list using json.dump.  \n",
    "\n",
    "The image dataset had the urls to the photos which came in handy to inspect some of the rows randomly where I realised some photos are not dogs, therefor excluded the rows with all False predictions on the photo being a dog, then filtered the rows where the predition probabilities weren't high enough.  \n",
    "\n",
    "The archive dataset had multiple columns with very low number of valid values so I dropped those columns. There were 2 rating columns for numerator and denominator where the values were often more than 10 but based on the project instructions I left them untouched and to combine them into 1 usefull column for analysis I divided the two and made a new column named \"score\".\n",
    "Also there were 4 columns for dog stage but sadly there were not many valid values in these columns but I combined them into 1 to be used in the analysis anyway.  \n",
    "\n",
    "After downloading the tweets using API, I changed the dataset type to pandas dataframe. There were many useless columns and I dropped all the ones that didn't seem usefull. Retweeted and favorite counts were the 2 important columns in the dataset.\n",
    "Also I cleaned the \"source\" column.  \n",
    "\n",
    "After cleaning the 3 datasets I used the twitter id columns to merge them all together and create a master dataset ready for analysis and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
